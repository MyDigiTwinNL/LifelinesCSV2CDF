# Lifelines CSV data files to JSON/CDF (Cohort-data format) transformation tool


Cohort studies play a crucial role in understanding the relationships between various factors and health outcomes over time. These studies collect extensive data from participants, including demographic information, clinical variables, lifestyle factors, and biomarkers. However, analyzing cohort study data often poses challenges, particularly when assessing variables across different time points. The data for the same variable is typically scattered across multiple files, each representing a specific assessment or follow-up visit. This fragmentation makes it difficult to perform comprehensive longitudinal analyses, or in the particular case of the MyDigiTwin project, to compute multiple points in time of the same variable to map it to standards like FHIR/MedMij.
 

This tool transforms cohort study data files in CSV (Comma-Separated Values) format into a format we called CDF/JSON (Cohort Data Format), which is already used by other data analysis tools in the MyDigiTwin project. A CDF format describes all the variables, and their values over time (i.e., each assessment), of an individual study participant. This format is particularly useful for the generation of FHIR/MedMij-compliant data (one of the aforementioned analysis tools). 

To illustrate what the tool does, consider the following three files, which contain values for the same variables (VariableX, VariableY, and VariableZ), collected in three different assessments (a1, a2, a3), for three participants (9f0.., 961...,e84...):

**a1_file.csv (first assessment datafile)**
```` 
PROJECT_PSEUDO_ID,VariableX,VariableY,VariableZ
"9f0f9676-9ec4-3","59","1","64"
"9618d9b2-65dd-3","77","1","49"
"e84e5122-f9d3-3","15","2","49"
````

**a2_file.csv (second assessment datafile)**
```` 
PROJECT_PSEUDO_ID,VariableX,VariableY,VariableZ
"9f0f9676-9ec4-3","67","1","31"
"9618d9b2-65dd-3","99","2","27"
"e84e5122-f9d3-3","59","1","19"
````

**a3_file.csv (third assessment datafile)**
```` 
PROJECT_PSEUDO_ID,VariableX,VariableY,VariableZ
"9f0f9676-9ec4-3","88","2","23"
"9618d9b2-65dd-3","3","2","64"
"e84e5122-f9d3-3","56","1","38"
````

Let's assume that only the variables VariableX and VariableZ are needed in the analysis. To perform the transformation, a configuration file, indicating in which file is each assessment of each variable, is needed:

**configuration.json**
````
{
    "VariableX": [{"a1":"a1_file.csv"},{"a2":"a2_file.csv"},{"a3":"a3_file.csv"}],
    "VariableZ": [{"a1":"a1_file.csv"},{"a2":"a2_file.csv"},{"a3":"a3_file.csv"}]
}
````

With the above configuration, the following JSON files would be generated by this tool:

````
{
    "PROJECT_PSEUDO_ID": {"1A": "9f0f9676-9ec4-3"}, 
    "VariableX": {"a1": "59", "a2": "67", "a3": "88"}, 
    "VariableZ": {"a1": "64", "a2": "31", "a3": "23"}
}

{
    "PROJECT_PSEUDO_ID": {"1A": "9618d9b2-65dd-3"}, 
    "VariableX": {"a1": "77", "a2": "99", "a3": "3"}, 
    "VariableZ": {"a1": "49", "a2": "27", "a3": "64"}
}

{
    "PROJECT_PSEUDO_ID": {"1A": "e84e5122-f9d3-3"}, 
    "VariableX": {"a1": "15", "a2": "59", "a3": "56"}, 
    "VariableZ": {"a1": "49", "a2": "19", "a3": "38"}
}
````

## Sample data files for a test run/benchmarking (memory/speed)

You can generate sample data files in the folder 'samplecsv/bigfiles' (folder included in the .gitignore). The following command generates N files, each one with C columns and R rows. By default, the first column will be 'PROJECT_PSEUDO_ID', and the following will be named 'Column1', 'Column2', ..., 'ColumnC'. For each row, a unique ID will be generated (but the same IDs will be used on each one of the N files):

````
    python samplecsv/generate_sample_csv_datafiles.py  N C R
````
For example, for generating ten CSV files, each one with 150000 rows and 200 columns (variables)

````
    python samplecsv/generate_sample_csv_datafiles.py  10 200 150000
````


## Data transformation

To transform the sample data files (or the actual files), you must first define the location of the different assessments of the variables you need to process. You can use the sample configuration files in 'sample-configs' for reference. You also need to provide a CSV file with all the IDs (participants) that are expected to be included in the transformations (the sample data generator previously mentioned generates one: 'samplecsv/bigfiles/pseudo_ids.csv')

````
    python lifelinescsv_to_icdf/cdfgenerator.py <file with ids> <config file> <output folder>

````





